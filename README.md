# Introduction

CS224N ìŠ¤í„°ë””ì˜ í•™ìŠµë‚´ìš© ì •ë¦¬ì™€ issueê³µìœ ë¥¼ ìœ„í•œ repoì…ë‹ˆë‹¤. 

- Stanford cs224n: Natural Language Processing with Deep Learning (**Winter 2019**)
  * [youtube](https://youtu.be/8rXD5-xhemo)
  * [syllabus, Winter 2019](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) // youtube version (ê¸°ë¡ ëª©ì°¨)
  * [syllabus, Winter 2020](http://web.stanford.edu/class/cs224n/)

- ğŸ“š **References:**
  - Dan Jurafsky and James H. Martin. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
  - Jacob Eisenstein. [Natural Language Processing](/src/eisenstein-nlp-notes.pdf)
  - Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](/src/A-Primer-on-Neural-Network-Models-for-Natural-Language-Processing.pdf)
  - Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org/)
  - PRML í•œêµ­ì–´ ë²ˆì—­/ì •ë¦¬: http://norman3.github.io/prml/


# Participant (alphabetical order)

| ì´ë¦„ | repo |
| :---: | :---: |
|ìœ ì¸í˜|[https://github.com/InhyeokYoo](https://github.com/InhyeokYoo) |
|ì¥ê±´í¬|https://github.com/ckrdkg |
|ìµœìŠ¬ê¸°|[github](https://github.com/abooundev)  |


# ê¸°ë¡ (20.10.13 - )

![ê³„íší‘œ](https://user-images.githubusercontent.com/47516855/97106526-8b0b9700-1705-11eb-8503-916730dcc116.png)


ğŸ’¡ **Importatn Note:**
- readmeì— ëŒ€í•œ ê´€ë¦¬ëŠ” ì œê°€ ê¸°ë³¸ì ìœ¼ë¡œ í•˜ê² ì§€ë§Œ, ê°ìì˜ ì‘ì—…ë¬¼ì„ pushí•˜ê³  ê´€ë¦¬í•˜ëŠ” ê±´ ê°ì ë¶€íƒë“œë¦½ë‹ˆë‹¤.
- ë§¤ì£¼ ìŠ¤í„°ë””ì—ì„œ ë‚˜ì˜¨ ì§ˆë¬¸/ê¶ê¸ˆì¦ ë“±ì€ issueë¡œ ë‚¨ê¸°ê³  labelì„ ë‹¬ì•„ì£¼ì„¸ìš”.
- ì—…ë°ì´íŠ¸ í•˜ê¸° ì „ì— fetch/pull ë¶€íƒë“œë¦½ë‹ˆë‹¤.
- ì–‘ì‹ì˜ í†µì¼ ë¶€íƒë“œë¦½ë‹ˆë‹¤ (markdown ì¶”ì²œ)
  - format ì¶”í›„ í˜‘ì˜
  - naming convention
    - ê°•ì˜ ì´ë¦„ìœ¼ë¡œ ëœ íŒŒì¼ì„ ì˜¬ë¦´ ê²ƒ
    - íŒŒì¼ ì´ë¦„ ê³µë°±ì€ `-`ë¡œ ëŒ€ì²´í•˜ì—¬ ì˜¬ë¦´ ê²ƒ
    - E.g. *Introduction-and-Word-Vectors.md*
- ìˆ˜ì‹ ë‚¨ê¸°ëŠ” ë°©ë²•:
  - `<img src="https://render.githubusercontent.com/render/math?math={w_1, ..., w_m}">`ì—ì„œ `math=`ì´í›„ì— ì‘ì„±.
  - ê²°ê³¼: <img src="https://render.githubusercontent.com/render/math?math={w_1, ..., w_m}">
- Noteì´ì˜ê²Œ ë§Œë“œëŠ” ë°©ë²•:

  ```markdown
  | :exclamation:  This is very important   |
  |-----------------------------------------|
  ```
  - ê²°ê³¼

    | :exclamation:  This is very important   |
    |-----------------------------------------|

ğŸ“‘ **ê°•ì˜ ëª©ì°¨:**  
| Weeks | 2020 | 2019 | ì¼ì¹˜ì—¬ë¶€ | ë°œí‘œì | ë°œí‘œë‚ ì§œ | ë§í¬ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
1 |  Introduction and Word Vectors/Gensim word vectors example |  | O | **ìœ ì¸í˜** | 2020.10.13 | [Link](/week1) |
2 | Word Vectors 2 and Word Senses |  | O | **ìœ ì¸í˜** | 2020.10.13 | [Link](/week1) |
3 | Word Window Classification, Neural Networks, and **PyTorch**  | Word Window Classification, Neural Networks, and Matrix Calculus | X |  **ìµœìŠ¬ê¸°** | 2020.10.21 | Link |
4 | Matrix Calculus and Backpropagation | Backpropagation and **Computation Graphs** | X |  **ì¥ê±´í¬** | 2020.10.21 | 2020.10.28 | Link |
5 | Linguistic Structure: Dependency Parsing  |  | O | 
6 | The probability of a sentence? Recurrent Neural Networks and Language Models | | O |  **ìœ ì¸í˜** | 2020.10.28 |  [Link](/week3/The-probability-of-a-sentence-Recurrent-Neural-Networks-and-Language-Models.md) |

## Week 3 (2020.10.28)

### 5. Linguistic Structure: Dependency Parsing

### 6. The probability of a sentence? Recurrent Neural Networks and Language Models

## Week 4 (ì§„í–‰ ì¤‘)

### 7. Vanishing Gradients and Fancy RNNs

### 8. Machine Translation, Seq2Seq and Attention

## Week 5 (ì§„í–‰ ì¤‘)

### 9. Practical Tips for Final Projects 

### 10. Question Answering and the Default Final Project

## Week 6 (ì§„í–‰ ì¤‘)

### 11. ConvNets for NLP 

### 12. Information from parts of words: Subword Models

## Week 7 (ì§„í–‰ ì¤‘)

### 13. Modeling contexts of use: Contextual Representations and Pretraining 

### 14. Transformers and Self-Attention For Generative Models

## Week 8 (ì§„í–‰ ì¤‘)

### 15. Natural Language Generation 

### 16. Reference in Language and Coreference Resolution

## Week 9 (ì§„í–‰ ì¤‘)

### 17. Multitask Learning: A general model for NLP?

### 18. Constituency Parsing and Tree Recursive Neural Networks 

### 19. Future of NLP + Deep Learning 


