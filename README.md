# Introduction

CS224N 스터디의 학습내용 정리와 issue공유를 위한 repo입니다. 

* Stanford cs224n: Natural Language Processing with Deep Learning (**Winter 2019**)
  * [youtube](https://youtu.be/8rXD5-xhemo)
  * [syllabus, Winter 2019](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) // youtube version (기록 목차)
  * [syllabus, Winter 2020](http://web.stanford.edu/class/cs224n/)

- References
  - Dan Jurafsky and James H. Martin. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
  - Jacob Eisenstein. [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
  - Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
  - Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org/)
  - 


# Participant (alphabetical order)

| 이름 | repo |
| :---: | :---: |
|유인혁|[https://github.com/InhyeokYoo](https://github.com/InhyeokYoo) |
|장건희|https://github.com/ckrdkg|
|최슬기|[github](https://github.com/abooundev)  |




# 기록 (20.10.13 - )

양식은 다음과 같이 써주세요.
- 기록에 link남겨주세요.
- 날짜도 남겨주세요
- 매주 스터디에서 나온 질문/궁금증 등은 issue로 남기고, 궁금증이 해결되면 issue를 닫겠습니다.
- 업데이트 하기 전에 fetch/pull 부탁드립니다.
- 추가적인 resource/material이 필요할지도 몰라 폴더로 만들었는데, markdown 내에 link/screenshot 등으로 남기면 될 것 같기도 하네요. 이 부분은 추후에 말씀나눠봐요.

## Week 1 (2020.10.13)

### 1. [Introduction and Word Vectors](/week1) (유인혁)

### 2. [Word Vectors 2 and Word Senses](/week1) (유인혁)


## Week 2 (2020.10.21)

### 3. [Word Window Classification, Neural Networks, and Matrix Calculus](/week2)

### 4. [Backpropagation and Computation Graphs](/week2)

## Week 3 (2020.10.28)

### 5. Linguistic Structure: Dependency Parsing

### 6. The probability of a sentence? Recurrent Neural Networks and Language Models

## Week 4 (진행 중)

### 7. Vanishing Gradients and Fancy RNNs

### 8. Machine Translation, Seq2Seq and Attention

## Week 5 (진행 중)

### 9. Practical Tips for Final Projects 

### 10. Question Answering and the Default Final Project

## Week 6 (진행 중)

### 11. ConvNets for NLP 

### 12. Information from parts of words: Subword Models

## Week 7 (진행 중)

### 13. Modeling contexts of use: Contextual Representations and Pretraining 

### 14. Transformers and Self-Attention For Generative Models

## Week 8 (진행 중)

### 15. Natural Language Generation 

### 16. Reference in Language and Coreference Resolution

## Week 9 (진행 중)

### 17. Multitask Learning: A general model for NLP?

### 18. Constituency Parsing and Tree Recursive Neural Networks 

### 19. Future of NLP + Deep Learning 


