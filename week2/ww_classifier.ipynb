{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (dglsql)",
      "language": "python",
      "name": "dglsql"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ww_classifier.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fStht8YFmnAY"
      },
      "source": [
        "## CS 224N Lecture 3: Word Window Classification\n",
        "\n",
        "### Pytorch Exploration\n",
        "\n",
        "### Author: Matthew Lamm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSoeY98EmnAa"
      },
      "source": [
        "import pprint\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "pp = pprint.PrettyPrinter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGZ6sU-mnAf"
      },
      "source": [
        "## Our Data\n",
        "\n",
        "The task at hand is to assign a label of 1 to words in a sentence that correspond with a LOCATION, and a label of 0 to everything else. \n",
        "\n",
        "In this simplified example, we only ever see spans of length 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "020Rv_0GnDDg",
        "outputId": "6654e10b-2347-43d2-de9f-52121fef6a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# 소문자로 변환 후, 각 sentence를 띄어쓰기 기준으로 분할\n",
        "train_sents = [s.lower().split() for s in [\"we 'll always have Paris\",\n",
        "                                           \"I live in Germany\",\n",
        "                                           \"He comes from Denmark\",\n",
        "                                           \"The capital of Denmark is Copenhagen\"]]\n",
        "train_sents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['we', \"'ll\", 'always', 'have', 'paris'],\n",
              " ['i', 'live', 'in', 'germany'],\n",
              " ['he', 'comes', 'from', 'denmark'],\n",
              " ['the', 'capital', 'of', 'denmark', 'is', 'copenhagen']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN5Jlz7etCuE"
      },
      "source": [
        "train_labels = [[0, 0, 0, 0, 1],\n",
        "                [0, 0, 0, 1],\n",
        "                [0, 0, 0, 1],\n",
        "                [0, 0, 0, 1, 0, 1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lixT8AzLnXUQ"
      },
      "source": [
        "assert all([len(train_sents[i]) == len(train_labels[i]) for i in range(len(train_sents))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdwHoIoNmnAk"
      },
      "source": [
        "test_sents = [s.lower().split() for s in [\"She comes from Paris\"]]\n",
        "test_labels = [[0, 0, 0, 1]]\n",
        "\n",
        "assert all([len(test_sents[i]) == len(test_labels[i]) for i in range(len(test_sents))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6qsIkmzmnAo"
      },
      "source": [
        "## Creating a dataset of batched tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H9KEVwcmnAo"
      },
      "source": [
        "PyTorch (like other deep learning frameworks) is optimized to work on __tensors__, which can be thought of as a generalization of vectors and matrices with arbitrarily large rank.\n",
        "\n",
        "Here well go over how to translate data to a list of vocabulary indices, and how to construct *batch tensors* out of the data for easy input to our model. \n",
        "\n",
        "We'll use the *torch.utils.data.DataLoader* object handle ease of batching and iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YLeO2zImnAp"
      },
      "source": [
        "### Converting tokenized sentence lists to vocabulary indices.\n",
        "\n",
        "Let's assume we have the following vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL6hlcKbmnAq",
        "outputId": "7c8282b4-60da-452e-b759-360af47684fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "id_2_word = [\"<pad>\", \"<unk>\", \"we\", \"always\", \"have\", \"paris\",\n",
        "              \"i\", \"live\", \"in\", \"germany\",\n",
        "              \"he\", \"comes\", \"from\", \"denmark\",\n",
        "              \"the\", \"of\", \"is\", \"copenhagen\"]\n",
        "id_2_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '<unk>',\n",
              " 'we',\n",
              " 'always',\n",
              " 'have',\n",
              " 'paris',\n",
              " 'i',\n",
              " 'live',\n",
              " 'in',\n",
              " 'germany',\n",
              " 'he',\n",
              " 'comes',\n",
              " 'from',\n",
              " 'denmark',\n",
              " 'the',\n",
              " 'of',\n",
              " 'is',\n",
              " 'copenhagen']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NQNILKRmnAt",
        "outputId": "e3ddb11e-2914-4db0-eacd-3cabf161d88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "instance = train_sents[0]\n",
        "print(instance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['we', \"'ll\", 'always', 'have', 'paris']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMTP4mP2g7Qx"
      },
      "source": [
        "word_2_id = {w:i for i,w in enumerate(id_2_word)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhWZGXv-mnAx"
      },
      "source": [
        "def convert_tokens_to_inds(sentence, word_2_id):\n",
        "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYQllSv3mnA0",
        "outputId": "8649cdc6-4beb-445a-8de1-332d9c8388be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_inds = convert_tokens_to_inds(instance, word_2_id)\n",
        "#pp.pprint(token_inds)\n",
        "token_inds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S4DUW3tmnA3"
      },
      "source": [
        "Let's convince ourselves that worked:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxTtrnBYmnA4",
        "outputId": "81569ff4-7476-4de9-ce03-ff424002792a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print([id_2_word[tok_idx] for tok_idx in token_inds])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['we', '<unk>', 'always', 'have', 'paris']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anDH-e9QmnA6"
      },
      "source": [
        "### Padding for windows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfSG96FmnA7"
      },
      "source": [
        "In the word window classifier, for each word in the sentence we want to get the +/- n window around the word, where 0 <= n < len(sentence).\n",
        "\n",
        "In order for such windows to be defined for words at the beginning and ends of the sentence, we actually want to insert padding around the sentence before converting to indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZzhkigYmnA7"
      },
      "source": [
        "#window_size만큼 앞,뒤로 padding\n",
        "def pad_sentence_for_window(sentence, window_size, pad_token=\"<pad>\"):\n",
        "    return [pad_token]*window_size + sentence + [pad_token]*window_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crxeQ7vGmnA-",
        "outputId": "cbc4e2da-063a-4c9f-ea49-0b2cf439d99d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "window_size = 2\n",
        "instance = pad_sentence_for_window(train_sents[0], window_size)\n",
        "print(instance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<pad>', '<pad>', 'we', \"'ll\", 'always', 'have', 'paris', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edrTLOBwmnBC"
      },
      "source": [
        "Let's make sure this works with our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "t4uJKXlTmnBC",
        "outputId": "7cf86406-3676-42cc-82cf-3d78fa1aca3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "for sent in train_sents:\n",
        "    tok_idxs = convert_tokens_to_inds(pad_sentence_for_window(sent, window_size), word_2_id)\n",
        "    print(sent)\n",
        "    print(\"sent len:\", len(sent))\n",
        "    print(tok_idxs)\n",
        "    print(\"tok len:\", len(tok_idxs))\n",
        "    print([id_2_word[idx] for idx in tok_idxs])\n",
        "    print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['we', \"'ll\", 'always', 'have', 'paris']\n",
            "sent len: 5\n",
            "[0, 0, 2, 1, 3, 4, 5, 0, 0]\n",
            "tok len: 9\n",
            "['<pad>', '<pad>', 'we', '<unk>', 'always', 'have', 'paris', '<pad>', '<pad>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "['i', 'live', 'in', 'germany']\n",
            "sent len: 4\n",
            "[0, 0, 6, 7, 8, 9, 0, 0]\n",
            "tok len: 8\n",
            "['<pad>', '<pad>', 'i', 'live', 'in', 'germany', '<pad>', '<pad>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "['he', 'comes', 'from', 'denmark']\n",
            "sent len: 4\n",
            "[0, 0, 10, 11, 12, 13, 0, 0]\n",
            "tok len: 8\n",
            "['<pad>', '<pad>', 'he', 'comes', 'from', 'denmark', '<pad>', '<pad>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "['the', 'capital', 'of', 'denmark', 'is', 'copenhagen']\n",
            "sent len: 6\n",
            "[0, 0, 14, 1, 15, 13, 16, 17, 0, 0]\n",
            "tok len: 10\n",
            "['<pad>', '<pad>', 'the', '<unk>', 'of', 'denmark', 'is', 'copenhagen', '<pad>', '<pad>']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_az4DDxKmnBF"
      },
      "source": [
        "### Batching sentences together with a DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QqR6HeYmnBG"
      },
      "source": [
        "When we train our model, we rarely update with respect to a single training instance at a time, because a single instance provides a very noisy estimate of the global loss's gradient. We instead construct small *batches* of data, and update parameters for each batch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlfobAiMmnBG"
      },
      "source": [
        "Given some batch size, we want to construct batch tensors out of the word index lists we've just created with our vocab.\n",
        "\n",
        "For each length B list of inputs, we'll have to:\n",
        "\n",
        "    (1) Add window padding to sentences in the batch like we just saw.\n",
        "    (2) Add additional padding so that each sentence in the batch is the same length.\n",
        "    (3) Make sure our labels are in the desired format.\n",
        "\n",
        "At the level of the dataest we want:\n",
        "\n",
        "    (4) Easy shuffling, because shuffling from one training epoch to the next gets rid of \n",
        "        pathological batches that are tough to learn from.\n",
        "    (5) Making sure we shuffle inputs and their labels together!\n",
        "    \n",
        "PyTorch provides us with an object *torch.utils.data.DataLoader* that gets us (4) and (5). All that's required of us is to specify a *collate_fn* that tells it how to do (1), (2), and (3). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL_bEcttwXT3",
        "outputId": "5fc07bf0-9971-4290-c7c8-c8406a01334d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMF2JVZfmnBH",
        "outputId": "39b01f52-f250-4305-de87-b15bc2a889b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "l = torch.LongTensor(train_labels[0])\n",
        "pp.pprint((\"raw train label instance\", l))\n",
        "print(l.size())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('raw train label instance', tensor([0, 0, 0, 0, 1]))\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUxYqLiamnBK",
        "outputId": "7af06faf-c56c-49bb-9d9c-8b82ebb1387e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "one_hots = torch.zeros((2, len(l)))\n",
        "pp.pprint((\"unfilled label instance\", one_hots))\n",
        "print(one_hots.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('unfilled label instance',\n",
            " tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]]))\n",
            "torch.Size([2, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwwJGxQmnBN",
        "outputId": "3aafd6ae-d39e-494c-bff7-8989607a9f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "one_hots[1] = l\n",
        "pp.pprint((\"one-hot labels\", one_hots))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('one-hot labels', tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GclPIZjwhcZ",
        "outputId": "a76cf837-3762-40b6-f785-1a7d756f5609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#0~255\n",
        "#\n",
        "~l.byte()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([255, 255, 255, 255, 254], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slSpUsJJmnBP",
        "outputId": "d9eec9e1-2b6d-450b-9a37-730f4b8eb955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "l_not = ~l.byte()\n",
        "one_hots[0] = l_not\n",
        "pp.pprint((\"one-hot labels\", one_hots))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('one-hot labels',\n",
            " tensor([[255., 255., 255., 255., 254.],\n",
            "        [  0.,   0.,   0.,   0.,   1.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKqNEvLBmnBS"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD-oCNWlmnBV"
      },
      "source": [
        "def my_collate(data, window_size, word_2_id):\n",
        "    \"\"\"\n",
        "    For some chunk of sentences and labels\n",
        "        -add winow padding\n",
        "        -pad for lengths using pad_sequence\n",
        "        -convert our labels to one-hots\n",
        "        -return padded inputs, one-hot labels, and lengths\n",
        "    \"\"\"\n",
        "    \n",
        "    x_s, y_s = zip(*data)\n",
        "    print(\"x_s:\", x_s) #(['we', \"'ll\", 'always', 'have', 'paris'], ['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'])\n",
        "    print(\"y_s:\", y_s) #([0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 1])\n",
        "\n",
        "    # deal with input sentences as we've seen\n",
        "    # window size만큼 앞, 뒤 padding -> 각 sentence마다 길이가 다름\n",
        "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id) \n",
        "                                                                                    for sentence in x_s]\n",
        "    print(\"window_padded:\", window_padded)\n",
        "\n",
        "    # append zeros to each list of token ids in batch so that they are all the same length\n",
        "    # batch가 가능하도록 같은 길이로 만들어 줌. batch 내 가장 긴 길이를 max로 선정. max보다 짧은 문장일 경우 padding \n",
        "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
        "    print(\"bathc_padded:\", padded)\n",
        "\n",
        "    # convert labels to one-hots\n",
        "    # label을 one-hots으로 인코딩? 무슨 변환? \n",
        "    \"\"\"\n",
        "    [0, 0, 0, 1] //4*1\n",
        "    -> [[255.,   0.], //4*2 -> 무슨 변환?\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]\n",
        "    -> [[255.,   0.], //5*2 -> batch 내 max 길이에 맞춰 padding\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.]]\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for y in y_s:\n",
        "        lengths.append(len(y))\n",
        "        label = torch.zeros((len(y),2 ))\n",
        "        true = torch.LongTensor(y) \n",
        "        false = ~true.byte()\n",
        "        label[:, 0] = false\n",
        "        label[:, 1] = true\n",
        "        labels.append(label)\n",
        "        print(\"y: {} -> label:{}\".format(y, label))\n",
        "    \n",
        "    # batch가 가능하도록 같은 길이로 만들어줌. batch 내 가장 긴 길이를 max로 선정. max보다 짧은 문장일 경우 padding \n",
        "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    print(\"padded_labels:\", padded_labels)\n",
        "    \n",
        "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqEiCCOTiQ0Z",
        "outputId": "943d988c-5885-41ef-dfec-9453f4a1e503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# sentence와 label을 zip()을 이용해 묶음. sentence list내 각 요소(한 문장)와 label list내 각 요소(그 문장에 대한 정답) set으로 묶어, list로 반환 \n",
        "list(zip(train_sents,train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['we', \"'ll\", 'always', 'have', 'paris'], [0, 0, 0, 0, 1]),\n",
              " (['i', 'live', 'in', 'germany'], [0, 0, 0, 1]),\n",
              " (['he', 'comes', 'from', 'denmark'], [0, 0, 0, 1]),\n",
              " (['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'], [0, 0, 0, 1, 0, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la5uUcHzmnBX"
      },
      "source": [
        "# Shuffle True is good practice for train loaders.\n",
        "# Use functools.partial to construct a partially populated collate function\n",
        "example_loader = DataLoader(list(zip(train_sents, \n",
        "                                    train_labels)), \n",
        "                                    batch_size=2, \n",
        "                                    shuffle=True, \n",
        "                                    collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLLNL4mmnBa",
        "outputId": "e32e3464-13b9-4ff9-8a22-b8bd858427da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for batched_input, batched_labels, batch_lengths in example_loader:\n",
        "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
        "    print(\"-\"*10)\n",
        "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
        "    print(\"-\"*10)\n",
        "    pp.pprint((\"batch_lengths\", batch_lengths))\n",
        "    print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_s: (['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'], ['we', \"'ll\", 'always', 'have', 'paris'])\n",
            "y_s: ([0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 1])\n",
            "window_padded: [[0, 0, 14, 1, 15, 13, 16, 17, 0, 0], [0, 0, 2, 1, 3, 4, 5, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
            "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0]])\n",
            "y: [0, 0, 0, 1, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]]])\n",
            "('inputs',\n",
            " tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
            "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0]]),\n",
            " torch.Size([2, 10]))\n",
            "----------\n",
            "('labels',\n",
            " tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]]]),\n",
            " torch.Size([2, 6, 2]))\n",
            "----------\n",
            "('batch_lengths', tensor([6, 5]))\n",
            "----------------------------------------------------------------------------------------------------\n",
            "x_s: (['he', 'comes', 'from', 'denmark'], ['i', 'live', 'in', 'germany'])\n",
            "y_s: ([0, 0, 0, 1], [0, 0, 0, 1])\n",
            "window_padded: [[0, 0, 10, 11, 12, 13, 0, 0], [0, 0, 6, 7, 8, 9, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "('inputs',\n",
            " tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0]]),\n",
            " torch.Size([2, 8]))\n",
            "----------\n",
            "('labels',\n",
            " tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]]),\n",
            " torch.Size([2, 4, 2]))\n",
            "----------\n",
            "('batch_lengths', tensor([4, 4]))\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPPE0dfmnBc"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "### Thinking through vectorization of word windows.\n",
        "Before we go ahead and build our model, let's think about the first thing it needs to do to its inputs.\n",
        "\n",
        "We're passed batches of sentences. For each sentence i in the batch, for each word j in the sentence, we want to construct a single tensor out of the embeddings surrounding word j in the +/- n window.\n",
        "\n",
        "Thus, the first thing we're going to need a (B, L, 2N+1) tensor of token indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z78e_6AjVlQ"
      },
      "source": [
        "한 batch 내 \n",
        "- 문장 인덱스: i\n",
        "- 문장 내 단어 인덱스: j\n",
        "- 윈도우 크기: n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FTI9TMNmnBd"
      },
      "source": [
        "A *terrible* but nevertheless informative *iterative* solution looks something like the following, where we iterate through batch elements in our (dummy), iterating non-padded word positions in those, and for each non-padded word position, construct a window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbobsS5qkFIQ",
        "outputId": "f9283811-dc84-4185-e5b6-490698387e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dummy_input = torch.zeros(2, 8).long()\n",
        "dummy_input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX22tpOkkKA6",
        "outputId": "c23ab77f-8fda-44fd-b0b4-478398f60211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.arange(1,9) #1부터 시작, 마지막 숫자-1까지 생성\n",
        "torch.arange(1,12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7jZcRQ2kNrN",
        "outputId": "19f3fb4c-3ee3-42ad-fae3-545ff4786e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# view(n,m) -> n*m의 개수가 앞의 차원수와 같아야 함\n",
        "torch.arange(1,9).view(2,4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4],\n",
              "        [5, 6, 7, 8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X-sJzKKmnBd",
        "outputId": "535de6d6-3c1a-4bf0-a249-956634150ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 양 옆에 window size=2만큼 padding\n",
        "dummy_input[:,2:-2] = torch.arange(1,9).view(2,4)\n",
        "pp.pprint(dummy_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 1, 2, 3, 4, 0, 0],\n",
            "        [0, 0, 5, 6, 7, 8, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGIOpug4k0R-",
        "outputId": "55a89264-2b25-441c-f1d4-399f001975bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dummy_output = [[[dummy_input[i, j-2+k].item() for k in range(2*2+1)] \n",
        "                                                     for j in range(2, 6)] \n",
        "                                                            for i in range(2)] \n",
        "dummy_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0, 0, 1, 2, 3], [0, 1, 2, 3, 4], [1, 2, 3, 4, 0], [2, 3, 4, 0, 0]],\n",
              " [[0, 0, 5, 6, 7], [0, 5, 6, 7, 8], [5, 6, 7, 8, 0], [6, 7, 8, 0, 0]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaMjtY35lFcT",
        "outputId": "d83928f5-7592-4646-f4d7-a7c549beb36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# 각 단어 중심으로 context=2로 설정하여 반환\n",
        "# batch는 2개씩 (batch내 각 요소는 문장)\n",
        "dummy_output = torch.LongTensor(dummy_output)\n",
        "print(dummy_output.size())\n",
        "pp.pprint(dummy_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 5])\n",
            "tensor([[[0, 0, 1, 2, 3],\n",
            "         [0, 1, 2, 3, 4],\n",
            "         [1, 2, 3, 4, 0],\n",
            "         [2, 3, 4, 0, 0]],\n",
            "\n",
            "        [[0, 0, 5, 6, 7],\n",
            "         [0, 5, 6, 7, 8],\n",
            "         [5, 6, 7, 8, 0],\n",
            "         [6, 7, 8, 0, 0]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi7eGck8mnBi"
      },
      "source": [
        "*Technically* it works: For each element in the batch, for each word in the original sentence and ignoring window padding, we've got the 5 token indices centered at that word. But in practice will be crazy slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3kCrCkdmnBi"
      },
      "source": [
        "Instead, we ideally want to find the right tensor operation in the PyTorch arsenal. Here, that happens to be __Tensor.unfold__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIj0EvBGmnBj",
        "outputId": "d9d4be04-088d-46a8-82ff-214980e0234a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "dummy_input.unfold(1, 2*2+1, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 0, 1, 2, 3],\n",
              "         [0, 1, 2, 3, 4],\n",
              "         [1, 2, 3, 4, 0],\n",
              "         [2, 3, 4, 0, 0]],\n",
              "\n",
              "        [[0, 0, 5, 6, 7],\n",
              "         [0, 5, 6, 7, 8],\n",
              "         [5, 6, 7, 8, 0],\n",
              "         [6, 7, 8, 0, 0]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jikNQVZmnBm"
      },
      "source": [
        "### A model in full."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSiqpbHXmnBn"
      },
      "source": [
        "In PyTorch, we implement models by extending the nn.Module class. Minimally, this requires implementing an *\\_\\_init\\_\\_* function and a *forward* function.\n",
        "\n",
        "In *\\_\\_init\\_\\_* we want to store model parameters (weights) and hyperparameters (dimensions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6ZioJsnPUs",
        "outputId": "2ff0d5e1-a5fb-4b27-9d27-f720c9c76de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Unit Test\n",
        "\"\"\"\n",
        "Embedding layer \n",
        "-model holds an embedding for each layer in our vocab\n",
        "-sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
        "-by default, embeddings are parameters (so gradients pass through them)\n",
        "\"\"\"\n",
        "vocab_size = 18\n",
        "embed_dim = 25\n",
        "pad_idx = 0\n",
        "freeze_embeddings = False\n",
        "embed_layer = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx) # 18x25 -> pad_idx???\n",
        "\n",
        "if freeze_embeddings:\n",
        "   embed_layer.weight.requires_grad = False\n",
        "\n",
        "print(\"embed_layer:\", embed_layer)\n",
        "print(\"embed_layer weights:\", embed_layer.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed_layer: Embedding(18, 25, padding_idx=0)\n",
            "embed_layer weights: Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000],\n",
            "        [ 0.1283,  0.4550,  0.5272, -1.3682,  0.5429, -1.9266, -0.0044,  0.0167,\n",
            "         -0.5833, -1.1719,  0.2619, -2.2598, -0.8888, -2.1736, -1.5393, -1.1363,\n",
            "         -2.2966, -0.6285, -1.4136,  0.9921, -0.7904, -0.1863, -1.2309, -1.3202,\n",
            "          0.4553],\n",
            "        [-0.6184,  0.7419, -2.1573,  0.4889,  1.5273, -1.5545,  1.4465,  0.2914,\n",
            "          0.8162, -0.6225, -0.7145, -0.4996, -1.3032,  0.6618, -0.5499,  0.4323,\n",
            "          0.2821, -0.6551,  2.2377, -0.1332, -0.4633,  0.8047, -0.6729,  1.7358,\n",
            "          0.4042],\n",
            "        [ 0.5512, -1.0479, -0.8720,  0.6667,  0.5202,  0.2001, -0.3286,  1.6962,\n",
            "          0.2439,  1.8662, -1.6066, -0.0498, -1.1968, -0.4987,  0.4731, -1.9852,\n",
            "         -0.5514, -0.2568, -0.0608,  0.3530,  0.7036, -1.2700,  0.2432,  0.4503,\n",
            "         -0.4524],\n",
            "        [ 1.5515,  0.4274, -1.1283, -0.0652,  0.2845, -0.4563,  0.2392,  0.1699,\n",
            "          0.6297,  0.3890,  0.3659, -0.0155,  1.4019,  2.2507, -0.6039, -1.6557,\n",
            "          0.3180, -0.7384,  0.3456,  1.1196,  0.7255,  0.7817,  1.4908,  0.0708,\n",
            "         -0.5040],\n",
            "        [ 0.3351,  0.6753,  0.8482, -0.6688, -0.7220, -0.7293, -0.9430,  0.1251,\n",
            "          0.9059, -0.7250, -1.9623,  0.3399,  1.0584,  1.5700, -0.3191,  1.3935,\n",
            "          0.1256,  0.6111, -0.0135, -1.4568, -1.5107,  0.9409, -0.2739,  0.0460,\n",
            "         -0.4776],\n",
            "        [-1.2751, -0.5279,  0.2588, -1.0057, -0.7157, -0.2294, -1.2485,  0.2316,\n",
            "          0.0907, -0.5005, -0.0242, -1.4289,  0.7826,  1.1483,  0.7746,  1.1002,\n",
            "         -0.1100, -0.4132,  0.5651, -0.4184, -1.5983, -0.1403, -0.8278,  0.9119,\n",
            "         -0.5287],\n",
            "        [-0.7001, -1.4097,  0.3670,  1.0219,  1.6277,  0.3122,  1.1018, -0.7045,\n",
            "          0.1705,  0.9074, -1.7035,  0.6063, -1.4515,  0.0730, -0.0239,  0.9613,\n",
            "          1.1361,  0.6428, -0.2988,  0.3981,  0.4111,  0.9099,  0.2530,  0.2713,\n",
            "          1.0099],\n",
            "        [-1.4796, -1.3814,  0.4796, -1.1133, -0.7385, -0.2551,  1.0374,  0.1726,\n",
            "         -1.5200,  0.5724, -0.3241, -0.6124,  0.3058,  0.7269, -0.7782, -0.1644,\n",
            "          0.0278, -0.3351, -0.5978, -0.1158,  0.9357, -0.0195, -0.3465, -0.1407,\n",
            "         -1.6808],\n",
            "        [ 0.7696, -0.8390, -0.2886,  0.5524, -1.6612, -0.9872, -0.5816, -0.2320,\n",
            "          0.6052, -1.6940,  0.0568,  0.0322,  0.4663, -0.0786, -0.2166,  2.3375,\n",
            "          0.4694, -0.1520, -0.6406, -0.0500, -0.0516, -1.2636, -2.8261,  1.1407,\n",
            "          0.2363],\n",
            "        [-1.5066,  1.2651,  0.6546, -1.7125, -0.4931, -0.3950, -0.3858,  0.1261,\n",
            "         -1.0196,  0.4561, -0.0859, -0.8186,  0.4273,  1.1866,  1.5827,  0.3296,\n",
            "          1.6799,  0.6898,  0.4244,  0.3053, -2.1357, -1.9743,  0.0329, -0.7339,\n",
            "          0.4038],\n",
            "        [ 0.3399, -1.0013,  1.0171, -0.2145, -0.0277, -0.7779,  1.1121,  1.5356,\n",
            "         -0.6533,  0.7186, -0.9287,  0.3200,  0.0301, -0.8800,  0.7830, -1.1383,\n",
            "         -0.7705, -1.6347, -0.5649,  0.2751, -1.9064, -1.2032,  1.6963, -0.3451,\n",
            "         -0.9195],\n",
            "        [-0.0558,  0.7142, -0.7775, -0.9086,  1.3877,  1.6156, -0.3067,  0.8614,\n",
            "          0.4392,  0.4908, -0.0244, -0.1231,  0.3475, -0.8620,  1.4184,  0.8519,\n",
            "          0.2995, -1.0171,  0.3585, -0.8322, -0.7635, -0.6550,  2.0795, -0.3428,\n",
            "          2.9504],\n",
            "        [-0.0519,  0.8210, -1.8051, -0.2184,  0.4616,  0.4235,  0.6663, -1.6861,\n",
            "          0.2278,  0.4968, -1.2523,  0.0299,  0.7427, -1.1602, -1.2052,  0.4241,\n",
            "         -0.0907, -0.3014, -0.0782,  2.2667,  0.3242, -1.2779, -0.4999, -0.0888,\n",
            "         -1.5539],\n",
            "        [ 0.9618, -1.6522,  0.0247,  0.0303, -0.4337, -0.5760,  0.2746,  0.3589,\n",
            "          1.1795,  0.4940,  0.6412, -1.0271, -0.4224, -1.6075,  1.1908, -0.5129,\n",
            "          1.1425, -0.1137,  0.7345,  1.9920,  0.2576,  1.0758, -1.9931, -0.4298,\n",
            "          0.9205],\n",
            "        [ 1.0894, -1.4198,  0.7634, -1.8294, -0.0992,  0.7282, -0.5529,  0.5116,\n",
            "         -0.5102, -0.7190,  0.2285,  0.5559, -2.0781, -1.2690,  0.8806,  0.2500,\n",
            "         -2.2028, -0.8094,  0.0888, -1.0695, -0.1902, -1.2146, -0.1747, -0.4472,\n",
            "          0.2618],\n",
            "        [-0.5413,  1.1339,  0.3734,  1.7401, -0.2068, -1.6728,  0.2978,  0.2072,\n",
            "         -0.8372,  1.3284,  0.0091, -0.2268,  0.4900,  1.6048, -1.1615,  0.2010,\n",
            "          0.5556,  1.0328,  0.6544, -0.0523,  0.0827,  1.6036, -0.0119, -0.4256,\n",
            "         -1.0545],\n",
            "        [-0.2243,  1.0491, -0.6354, -1.1468,  0.3298, -0.5919,  1.2389, -0.4797,\n",
            "         -1.3344,  0.7195,  1.7852,  1.2920, -0.1617, -1.6737,  0.9197,  0.6160,\n",
            "         -0.3586,  0.7932,  1.5153,  0.0316,  0.5515, -0.1782,  2.3984,  1.7021,\n",
            "          0.6770]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbUryUTJX_kA",
        "outputId": "62cc674d-1d60-4b53-ef82-cf121189cb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "#Unit Test\n",
        "\"\"\"\n",
        "Hidden layer\n",
        "-we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
        "-nn.Sequential allows you to efficiently specify sequentially structured models\n",
        "    -first the linear transformation is evoked on the embedded word windows\n",
        "    -next the nonlinear transformation tanh is evoked.\n",
        "\"\"\"\n",
        "window_size = 5\n",
        "embed_dim = 25\n",
        "hidden_dim = 25\n",
        "linear = nn.Linear(window_size*embed_dim,hidden_dim)\n",
        "hidden_layer = nn.Sequential(nn.Linear(window_size*embed_dim,hidden_dim),nn.Tanh())\n",
        "print(linear.weight)\n",
        "print(linear.weight.size()) # 25x125"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0783,  0.0529,  0.0459,  ..., -0.0060, -0.0758,  0.0245],\n",
            "        [ 0.0469, -0.0776,  0.0787,  ...,  0.0096, -0.0386, -0.0562],\n",
            "        [-0.0655, -0.0791,  0.0295,  ..., -0.0405,  0.0273,  0.0667],\n",
            "        ...,\n",
            "        [-0.0437,  0.0461,  0.0226,  ...,  0.0835, -0.0285,  0.0582],\n",
            "        [-0.0269, -0.0240,  0.0064,  ..., -0.0045, -0.0094,  0.0382],\n",
            "        [ 0.0639, -0.0365, -0.0192,  ...,  0.0361,  0.0224,  0.0671]],\n",
            "       requires_grad=True)\n",
            "torch.Size([25, 125])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDMS56-ymt1f",
        "outputId": "e3f6ae9f-abc8-4ba1-f2ba-3999b01e8190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "L:= window-padded sentence length\n",
        "S:= window_size = 2*half_window\"+1 = 5\n",
        "\n",
        "config = {\"batch_size\": 4, #B\n",
        "          \"half_window\": 2, \n",
        "          \"embed_dim\": 25,  #D\n",
        "          \"hidden_dim\": 25, #H\n",
        "          \"num_classes\": 2,\n",
        "          \"freeze_embeddings\": False,\n",
        "         }\n",
        "learning_rate = .0002\n",
        "num_epochs = 10000\n",
        "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nconfig = {\"batch_size\": 4,\\n          \"half_window\": 2,\\n          \"embed_dim\": 25,\\n          \"hidden_dim\": 25,\\n          \"num_classes\": 2,\\n          \"freeze_embeddings\": False,\\n         }\\nlearning_rate = .0002\\nnum_epochs = 10000\\nmodel = SoftmaxWordWindowClassifier(config, len(word_2_id))\\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avJ4gfZkuNuJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lmskplPmnBo"
      },
      "source": [
        "\"\"\"\n",
        "   Let B:= batch_size\n",
        "      L:= window-padded sentence length\n",
        "      D:= self.embed_dim\n",
        "      S:= self.window_size\n",
        "      H:= self.hidden_dim\n",
        "\"\"\"\n",
        "class SoftmaxWordWindowClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A one-layer, binary word-window classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, vocab_size, pad_idx=0):\n",
        "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
        "        \"\"\"\n",
        "        Instance variables.\n",
        "        \"\"\"\n",
        "        self.window_size = 2*config[\"half_window\"]+1 # window size는 5\n",
        "        self.embed_dim = config[\"embed_dim\"] # 25차원으로 embedding\n",
        "        self.hidden_dim = config[\"hidden_dim\"] # 은닉층 25units\n",
        "        self.num_classes = config[\"num_classes\"] # 2classes 분류\n",
        "        self.freeze_embeddings = config[\"freeze_embeddings\"] # False\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding layer\n",
        "        -model holds an embedding for each layer in our vocab\n",
        "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
        "        -by default, embeddings are parameters (so gradients pass through them)\n",
        "        \"\"\"\n",
        "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
        "        if self.freeze_embeddings:\n",
        "            self.embed_layer.weight.requires_grad = False\n",
        "        \n",
        "        \"\"\"\n",
        "        Hidden layer\n",
        "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
        "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
        "            -first the linear transformation is evoked on the embedded word windows\n",
        "            -next the nonlinear transformation tanh is evoked.\n",
        "        \"\"\"\n",
        "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
        "                                                    self.hidden_dim), \n",
        "                                          nn.Tanh())\n",
        "        \n",
        "        \"\"\"\n",
        "        Output layer\n",
        "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
        "        \"\"\"\n",
        "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax\n",
        "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
        "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
        "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
        "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
        "         is optimized to avoid numerical underflow issues.\n",
        "        \"\"\"\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Let B:= batch_size\n",
        "            L:= window-padded sentence length\n",
        "            D:= self.embed_dim\n",
        "            S:= self.window_size\n",
        "            H:= self.hidden_dim\n",
        "            \n",
        "        inputs: a (B, L) tensor of token indices\n",
        "        \"\"\"\n",
        "        B, L = inputs.size()\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L) LongTensor\n",
        "        Outputs a (B, L~, S) LongTensor\n",
        "        \"\"\"\n",
        "        # Fist, get our word windows for each word in our input.\n",
        "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
        "        _, adjusted_length, _ = token_windows.size()\n",
        "        \n",
        "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
        "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding.\n",
        "        Takes in a torch.LongTensor of size (B, L~, S) \n",
        "        Outputs a (B, L~, S, D) FloatTensor.\n",
        "        \"\"\"\n",
        "        embedded_windows = self.embed_layer(token_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L~, S, D) FloatTensor.\n",
        "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
        "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
        "        \"\"\"\n",
        "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 1.\n",
        "        Takes in a (B, L~, S*D) FloatTensor.\n",
        "        Resizes it into a (B, L~, H) FloatTensor\n",
        "        \"\"\"\n",
        "        layer_1 = self.hidden_layer(embedded_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 2\n",
        "        Takes in a (B, L~, H) FloatTensor.\n",
        "        Resizes it into a (B, L~, 2) FloatTensor.\n",
        "        \"\"\"\n",
        "        output = self.output_layer(layer_1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax.\n",
        "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
        "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
        "        \"\"\"\n",
        "        output = self.log_softmax(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42gqlC2mmnBq"
      },
      "source": [
        "### Training.\n",
        "\n",
        "Now that we've got a model, we have to train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKTUKEXmnBr"
      },
      "source": [
        "def loss_function(outputs, labels, lengths):\n",
        "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
        "    B, L, num_classes = outputs.size()\n",
        "    num_elems = lengths.sum().float()\n",
        "        \n",
        "    # get only the values with non-zero labels\n",
        "    loss = outputs*labels\n",
        "    \n",
        "    # rescale average\n",
        "    return -loss.sum() / num_elems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipMSfqNfmnBt"
      },
      "source": [
        "def train_epoch(loss_function, optimizer, model, train_data):\n",
        "    \n",
        "    ## For each batch, we must reset the gradients\n",
        "    ## stored by the model.   \n",
        "    total_loss = 0\n",
        "    idx_batch = 0\n",
        "    # dataloader에서 batch 단위로 data 반환\n",
        "    for batch, labels, lengths in train_data: #  train_data: torch.utils.data.DataLoader\n",
        "        print(\"{}번째 batch:\".format(idx_batch))\n",
        "        print(\"batch:\", batch)\n",
        "        print(\"labels:\", labels)\n",
        "        print(\"lengths:\", lengths)\n",
        "\n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # evoke model in training mode on batch\n",
        "        # batch별 classifier모델에 넣어서 output 계산\n",
        "        outputs = model.forward(batch)\n",
        "        print(\"batch outputs:\", outputs)\n",
        "\n",
        "        # compute loss w.r.t batch\n",
        "        # batch 별 loss 계산\n",
        "        loss = loss_function(outputs, labels, lengths)\n",
        "        print(\"batch loss:\", loss)\n",
        "        \n",
        "        # pass gradients back, startiing on loss value\n",
        "        # backpropagation을 위한 기울기 전달\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters\n",
        "        #optimizer가 매개변수 업데이트\n",
        "        optimizer.step() \n",
        "\n",
        "        total_loss += loss.item()\n",
        "        idx_batch += 1\n",
        "        print(\"=\"*100)\n",
        "    \n",
        "    # return the total to keep track of how you did this time around\n",
        "    return total_loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTXAvCiumnBv"
      },
      "source": [
        "config = {\"batch_size\": 4,\n",
        "          \"half_window\": 2,\n",
        "          \"embed_dim\": 25,\n",
        "          \"hidden_dim\": 25,\n",
        "          \"num_classes\": 2,\n",
        "          \"freeze_embeddings\": False,\n",
        "         }\n",
        "learning_rate = .0002\n",
        "num_epochs = 2#10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13L5MTvMtrG5",
        "outputId": "97f6be4c-7ee5-44eb-fbfb-86f9ac52a4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "#vocab_size = 18\n",
        "len(word_2_id)\n",
        "word_2_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<unk>': 1,\n",
              " 'always': 3,\n",
              " 'comes': 11,\n",
              " 'copenhagen': 17,\n",
              " 'denmark': 13,\n",
              " 'from': 12,\n",
              " 'germany': 9,\n",
              " 'have': 4,\n",
              " 'he': 10,\n",
              " 'i': 6,\n",
              " 'in': 8,\n",
              " 'is': 16,\n",
              " 'live': 7,\n",
              " 'of': 15,\n",
              " 'paris': 5,\n",
              " 'the': 14,\n",
              " 'we': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQ8VPp2toH6"
      },
      "source": [
        "model = SoftmaxWordWindowClassifier(config, len(word_2_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InRu63zqt0O7",
        "outputId": "d8379acd-4390-46d8-ecb9-e87e1bed515b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " model.parameters() #type: generator -> 값 확인하기???"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generator"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT9-bZHZtoPQ",
        "outputId": "0859e633-2159-44d9-9093-94ac3a5e565a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# optimizer에 모델의 매개변수 전달\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 0.0002\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLGGSkA1mnBy"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(list(zip(train_sents, train_labels)), \n",
        "                                           batch_size=2, \n",
        "                                           shuffle=True, \n",
        "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "BhivplHqmnB0",
        "outputId": "6e4ad45e-005d-47a0-ba79-f3c61e98f912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"{}번째 epoch:\".format(epoch))\n",
        "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
        "    if epoch % 100 == 0:\n",
        "        losses.append(epoch_loss)\n",
        "    print(\"*\"*100)\n",
        "print(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0번째 epoch:\n",
            "x_s: (['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'], ['i', 'live', 'in', 'germany'])\n",
            "y_s: ([0, 0, 0, 1, 0, 1], [0, 0, 0, 1])\n",
            "window_padded: [[0, 0, 14, 1, 15, 13, 16, 17, 0, 0], [0, 0, 6, 7, 8, 9, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0,  0,  0]])\n",
            "y: [0, 0, 0, 1, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.],\n",
            "         [  0.,   0.]]])\n",
            "0번째 batch:\n",
            "batch: tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0,  0,  0]])\n",
            "labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.],\n",
            "         [  0.,   0.]]])\n",
            "lengths: tensor([6, 4])\n",
            "batch outputs: tensor([[[-0.5967, -0.7999],\n",
            "         [-0.5707, -0.8326],\n",
            "         [-1.1202, -0.3948],\n",
            "         [-0.4604, -0.9971],\n",
            "         [-0.4353, -1.0414],\n",
            "         [-0.8197, -0.5808]],\n",
            "\n",
            "        [[-0.6273, -0.7637],\n",
            "         [-0.4523, -1.0111],\n",
            "         [-0.8326, -0.5707],\n",
            "         [-0.9057, -0.5179],\n",
            "         [-0.7767, -0.6160],\n",
            "         [-0.9085, -0.5160]]], grad_fn=<LogSoftmaxBackward>)\n",
            "batch loss: tensor(173.9259, grad_fn=<DivBackward0>)\n",
            "====================================================================================================\n",
            "x_s: (['he', 'comes', 'from', 'denmark'], ['we', \"'ll\", 'always', 'have', 'paris'])\n",
            "y_s: ([0, 0, 0, 1], [0, 0, 0, 0, 1])\n",
            "window_padded: [[0, 0, 10, 11, 12, 13, 0, 0], [0, 0, 2, 1, 3, 4, 5, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0,  0],\n",
            "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "1번째 batch:\n",
            "batch: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0,  0],\n",
            "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0]])\n",
            "labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "lengths: tensor([4, 5])\n",
            "batch outputs: tensor([[[-0.7121, -0.6745],\n",
            "         [-0.8696, -0.5432],\n",
            "         [-0.6325, -0.7577],\n",
            "         [-0.6608, -0.7266],\n",
            "         [-0.6235, -0.7680]],\n",
            "\n",
            "        [[-0.5920, -0.8057],\n",
            "         [-0.4362, -1.0398],\n",
            "         [-0.6645, -0.7227],\n",
            "         [-0.5730, -0.8298],\n",
            "         [-0.7106, -0.6760]]], grad_fn=<LogSoftmaxBackward>)\n",
            "batch loss: tensor(165.7885, grad_fn=<DivBackward0>)\n",
            "====================================================================================================\n",
            "****************************************************************************************************\n",
            "1번째 epoch:\n",
            "x_s: (['we', \"'ll\", 'always', 'have', 'paris'], ['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'])\n",
            "y_s: ([0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 1])\n",
            "window_padded: [[0, 0, 2, 1, 3, 4, 5, 0, 0], [0, 0, 14, 1, 15, 13, 16, 17, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0],\n",
            "        [ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0]])\n",
            "y: [0, 0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 1, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "0번째 batch:\n",
            "batch: tensor([[ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0],\n",
            "        [ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0]])\n",
            "labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [  0.,   0.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "lengths: tensor([5, 6])\n",
            "batch outputs: tensor([[[-0.5351, -0.8809],\n",
            "         [-0.3822, -1.1468],\n",
            "         [-0.5705, -0.8330],\n",
            "         [-0.4945, -0.9413],\n",
            "         [-0.6319, -0.7584],\n",
            "         [-0.7449, -0.6439]],\n",
            "\n",
            "        [[-0.5207, -0.9017],\n",
            "         [-0.4983, -0.9353],\n",
            "         [-0.9472, -0.4908],\n",
            "         [-0.3760, -1.1602],\n",
            "         [-0.3984, -1.1129],\n",
            "         [-0.6978, -0.6885]]], grad_fn=<LogSoftmaxBackward>)\n",
            "batch loss: tensor(140.3917, grad_fn=<DivBackward0>)\n",
            "====================================================================================================\n",
            "x_s: (['he', 'comes', 'from', 'denmark'], ['i', 'live', 'in', 'germany'])\n",
            "y_s: ([0, 0, 0, 1], [0, 0, 0, 1])\n",
            "window_padded: [[0, 0, 10, 11, 12, 13, 0, 0], [0, 0, 6, 7, 8, 9, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "1번째 batch:\n",
            "batch: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0]])\n",
            "labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]],\n",
            "\n",
            "        [[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "lengths: tensor([4, 4])\n",
            "batch outputs: tensor([[[-0.5954, -0.8014],\n",
            "         [-0.7166, -0.6702],\n",
            "         [-0.4940, -0.9421],\n",
            "         [-0.5418, -0.8715]],\n",
            "\n",
            "        [[-0.5155, -0.9094],\n",
            "         [-0.3853, -1.1402],\n",
            "         [-0.6989, -0.6874],\n",
            "         [-0.7854, -0.6087]]], grad_fn=<LogSoftmaxBackward>)\n",
            "batch loss: tensor(150.8819, grad_fn=<DivBackward0>)\n",
            "====================================================================================================\n",
            "****************************************************************************************************\n",
            "[339.7144470214844]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxUERogBpvXn"
      },
      "source": [
        "\"\"\"\n",
        "0번째 epoch:\n",
        "x_s: (['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'], ['i', 'live', 'in', 'germany'])\n",
        "y_s: ([0, 0, 0, 1, 0, 1], [0, 0, 0, 1])\n",
        "window_padded: [[0, 0, 14, 1, 15, 13, 16, 17, 0, 0], [0, 0, 6, 7, 8, 9, 0, 0]]\n",
        "bathc_padded: tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
        "        [ 0,  0,  6,  7,  8,  9,  0,  0,  0,  0]])\n",
        "y: [0, 0, 0, 1, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "padded_labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.],\n",
        "         [  0.,   0.]]])\n",
        "0번째 batch:\n",
        "batch: tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
        "        [ 0,  0,  6,  7,  8,  9,  0,  0,  0,  0]])\n",
        "labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.],\n",
        "         [  0.,   0.]]])\n",
        "lengths: tensor([6, 4])\n",
        "batch outputs: tensor([[[-0.5967, -0.7999],\n",
        "         [-0.5707, -0.8326],\n",
        "         [-1.1202, -0.3948],\n",
        "         [-0.4604, -0.9971],\n",
        "         [-0.4353, -1.0414],\n",
        "         [-0.8197, -0.5808]],\n",
        "\n",
        "        [[-0.6273, -0.7637],\n",
        "         [-0.4523, -1.0111],\n",
        "         [-0.8326, -0.5707],\n",
        "         [-0.9057, -0.5179],\n",
        "         [-0.7767, -0.6160],\n",
        "         [-0.9085, -0.5160]]], grad_fn=<LogSoftmaxBackward>)\n",
        "batch loss: tensor(173.9259, grad_fn=<DivBackward0>)\n",
        "====================================================================================================\n",
        "x_s: (['he', 'comes', 'from', 'denmark'], ['we', \"'ll\", 'always', 'have', 'paris'])\n",
        "y_s: ([0, 0, 0, 1], [0, 0, 0, 0, 1])\n",
        "window_padded: [[0, 0, 10, 11, 12, 13, 0, 0], [0, 0, 2, 1, 3, 4, 5, 0, 0]]\n",
        "bathc_padded: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0,  0],\n",
        "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0]])\n",
        "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "y: [0, 0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "padded_labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "1번째 batch:\n",
        "batch: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0,  0],\n",
        "        [ 0,  0,  2,  1,  3,  4,  5,  0,  0]])\n",
        "labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "lengths: tensor([4, 5])\n",
        "batch outputs: tensor([[[-0.7121, -0.6745],\n",
        "         [-0.8696, -0.5432],\n",
        "         [-0.6325, -0.7577],\n",
        "         [-0.6608, -0.7266],\n",
        "         [-0.6235, -0.7680]],\n",
        "\n",
        "        [[-0.5920, -0.8057],\n",
        "         [-0.4362, -1.0398],\n",
        "         [-0.6645, -0.7227],\n",
        "         [-0.5730, -0.8298],\n",
        "         [-0.7106, -0.6760]]], grad_fn=<LogSoftmaxBackward>)\n",
        "batch loss: tensor(165.7885, grad_fn=<DivBackward0>)\n",
        "====================================================================================================\n",
        "****************************************************************************************************\n",
        "1번째 epoch:\n",
        "x_s: (['we', \"'ll\", 'always', 'have', 'paris'], ['the', 'capital', 'of', 'denmark', 'is', 'copenhagen'])\n",
        "y_s: ([0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 1])\n",
        "window_padded: [[0, 0, 2, 1, 3, 4, 5, 0, 0], [0, 0, 14, 1, 15, 13, 16, 17, 0, 0]]\n",
        "bathc_padded: tensor([[ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0],\n",
        "        [ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0]])\n",
        "y: [0, 0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "y: [0, 0, 0, 1, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "padded_labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "0번째 batch:\n",
        "batch: tensor([[ 0,  0,  2,  1,  3,  4,  5,  0,  0,  0],\n",
        "        [ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0]])\n",
        "labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [  0.,   0.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "lengths: tensor([5, 6])\n",
        "batch outputs: tensor([[[-0.5351, -0.8809],\n",
        "         [-0.3822, -1.1468],\n",
        "         [-0.5705, -0.8330],\n",
        "         [-0.4945, -0.9413],\n",
        "         [-0.6319, -0.7584],\n",
        "         [-0.7449, -0.6439]],\n",
        "\n",
        "        [[-0.5207, -0.9017],\n",
        "         [-0.4983, -0.9353],\n",
        "         [-0.9472, -0.4908],\n",
        "         [-0.3760, -1.1602],\n",
        "         [-0.3984, -1.1129],\n",
        "         [-0.6978, -0.6885]]], grad_fn=<LogSoftmaxBackward>)\n",
        "batch loss: tensor(140.3917, grad_fn=<DivBackward0>)\n",
        "====================================================================================================\n",
        "x_s: (['he', 'comes', 'from', 'denmark'], ['i', 'live', 'in', 'germany'])\n",
        "y_s: ([0, 0, 0, 1], [0, 0, 0, 1])\n",
        "window_padded: [[0, 0, 10, 11, 12, 13, 0, 0], [0, 0, 6, 7, 8, 9, 0, 0]]\n",
        "bathc_padded: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
        "        [ 0,  0,  6,  7,  8,  9,  0,  0]])\n",
        "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [255.,   0.],\n",
        "        [254.,   1.]])\n",
        "padded_labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "1번째 batch:\n",
        "batch: tensor([[ 0,  0, 10, 11, 12, 13,  0,  0],\n",
        "        [ 0,  0,  6,  7,  8,  9,  0,  0]])\n",
        "labels: tensor([[[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]],\n",
        "\n",
        "        [[255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [255.,   0.],\n",
        "         [254.,   1.]]])\n",
        "lengths: tensor([4, 4])\n",
        "batch outputs: tensor([[[-0.5954, -0.8014],\n",
        "         [-0.7166, -0.6702],\n",
        "         [-0.4940, -0.9421],\n",
        "         [-0.5418, -0.8715]],\n",
        "\n",
        "        [[-0.5155, -0.9094],\n",
        "         [-0.3853, -1.1402],\n",
        "         [-0.6989, -0.6874],\n",
        "         [-0.7854, -0.6087]]], grad_fn=<LogSoftmaxBackward>)\n",
        "batch loss: tensor(150.8819, grad_fn=<DivBackward0>)\n",
        "====================================================================================================\n",
        "****************************************************************************************************\n",
        "[339.7144470214844]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVMO291mnB3"
      },
      "source": [
        "### Prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nnv2PnHmnB3",
        "outputId": "13bef47d-a3e4-421e-c62f-26de17e3ebc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(list(zip(test_sents, test_labels)), \n",
        "                                           batch_size=1, \n",
        "                                           shuffle=False, \n",
        "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n",
        "test_loader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f63349a1eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "465iEDJisArI",
        "outputId": "e0437b4b-7809-456f-82fd-2abff503e6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "for batched_input, batched_labels, batch_lengths in test_loader:\n",
        "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
        "    print(\"-\"*10)\n",
        "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
        "    print(\"-\"*10)\n",
        "    pp.pprint((\"batch_lengths\", batch_lengths))\n",
        "    print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_s: (['she', 'comes', 'from', 'paris'],)\n",
            "y_s: ([0, 0, 0, 1],)\n",
            "window_padded: [[0, 0, 1, 11, 12, 5, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0,  1, 11, 12,  5,  0,  0]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "('inputs', tensor([[ 0,  0,  1, 11, 12,  5,  0,  0]]), torch.Size([1, 8]))\n",
            "----------\n",
            "('labels',\n",
            " tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]]),\n",
            " torch.Size([1, 4, 2]))\n",
            "----------\n",
            "('batch_lengths', tensor([4]))\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6C8Xx4YmnB6",
        "outputId": "47ca9e94-4c3f-49b0-c01f-3f31118ce4bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "for test_instance, labs, _ in test_loader:\n",
        "    print(\"test instance:\", test_instance)\n",
        "    print(\"test labs\", labs)\n",
        "    outputs = model.forward(test_instance)\n",
        "    print(\"test outputs:\", outputs)\n",
        "\n",
        "    print(\"argmax(outputs):\", torch.argmax(outputs, dim=2))\n",
        "    print(\"argmax(labs):\", torch.argmax(labs, dim=2))\n",
        "    print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_s: (['she', 'comes', 'from', 'paris'],)\n",
            "y_s: ([0, 0, 0, 1],)\n",
            "window_padded: [[0, 0, 1, 11, 12, 5, 0, 0]]\n",
            "bathc_padded: tensor([[ 0,  0,  1, 11, 12,  5,  0,  0]])\n",
            "y: [0, 0, 0, 1] -> label:tensor([[255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [255.,   0.],\n",
            "        [254.,   1.]])\n",
            "padded_labels: tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "test instance: tensor([[ 0,  0,  1, 11, 12,  5,  0,  0]])\n",
            "test labs tensor([[[255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [255.,   0.],\n",
            "         [254.,   1.]]])\n",
            "test outputs: tensor([[[-0.5031, -0.9280],\n",
            "         [-0.7167, -0.6702],\n",
            "         [-0.3982, -1.1134],\n",
            "         [-0.4939, -0.9422]]], grad_fn=<LogSoftmaxBackward>)\n",
            "argmax(outputs): tensor([[0, 1, 0, 0]])\n",
            "argmax(labs): tensor([[0, 0, 0, 0]])\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtgSIhkBmnB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}