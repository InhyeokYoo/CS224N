# Introduction

CS224N ìŠ¤í„°ë””ì˜ í•™ìŠµë‚´ìš© ì •ë¦¬ì™€ issueê³µìœ ë¥¼ ìœ„í•œ repoì…ë‹ˆë‹¤. 

- Stanford cs224n: Natural Language Processing with Deep Learning (**Winter 2019**)
  * [youtube](https://youtu.be/8rXD5-xhemo)
  * [syllabus, Winter 2019](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) // youtube version (ê¸°ë¡ ëª©ì°¨)
  * [syllabus, Winter 2020](http://web.stanford.edu/class/cs224n/)

- ğŸ“š**References:**
  - Dan Jurafsky and James H. Martin. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
  - Jacob Eisenstein. [Natural Language Processing](src/A Primer on Neural Network Modelsfor Natural Language Processing.pdf)
  - Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
  - Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org/)
  - PRML í•œêµ­ì–´ ë²ˆì—­/ì •ë¦¬: http://norman3.github.io/prml/



# Participant (alphabetical order)

| ì´ë¦„ | repo |
| :---: | :---: |
|ìœ ì¸í˜|[https://github.com/InhyeokYoo](https://github.com/InhyeokYoo) |
|ì¥ê±´í¬|https://github.com/ckrdkg|
|ìµœìŠ¬ê¸°|[github](https://github.com/abooundev)  |




# ê¸°ë¡ (20.10.13 - )

ì–‘ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì¨ì£¼ì„¸ìš”.
- ê¸°ë¡ì— linkë‚¨ê²¨ì£¼ì„¸ìš”.
- ë‚ ì§œë„ ë‚¨ê²¨ì£¼ì„¸ìš”
- ë§¤ì£¼ ìŠ¤í„°ë””ì—ì„œ ë‚˜ì˜¨ ì§ˆë¬¸/ê¶ê¸ˆì¦ ë“±ì€ issueë¡œ ë‚¨ê¸°ê³ , ê¶ê¸ˆì¦ì´ í•´ê²°ë˜ë©´ issueë¥¼ ë‹«ê² ìŠµë‹ˆë‹¤.
- ì—…ë°ì´íŠ¸ í•˜ê¸° ì „ì— fetch/pull ë¶€íƒë“œë¦½ë‹ˆë‹¤.
- ì¶”ê°€ì ì¸ resource/materialì´ í•„ìš”í• ì§€ë„ ëª°ë¼ í´ë”ë¡œ ë§Œë“¤ì—ˆëŠ”ë°, markdown ë‚´ì— link/screenshot ë“±ìœ¼ë¡œ ë‚¨ê¸°ë©´ ë  ê²ƒ ê°™ê¸°ë„ í•˜ë„¤ìš”. ì´ ë¶€ë¶„ì€ ì¶”í›„ì— ë§ì”€ë‚˜ëˆ ë´ìš”.

## Week 1 (2020.10.13)

### 1. [Introduction and Word Vectors](/week1) (ìœ ì¸í˜)

### 2. [Word Vectors 2 and Word Senses](/week1) (ìœ ì¸í˜)


## Week 2 (2020.10.21)

### 3. [Word Window Classification, Neural Networks, and Matrix Calculus](/week2)

### 4. [Backpropagation and Computation Graphs](/week2)

## Week 3 (2020.10.28)

### 5. Linguistic Structure: Dependency Parsing

### 6. The probability of a sentence? Recurrent Neural Networks and Language Models

## Week 4 (ì§„í–‰ ì¤‘)

### 7. Vanishing Gradients and Fancy RNNs

### 8. Machine Translation, Seq2Seq and Attention

## Week 5 (ì§„í–‰ ì¤‘)

### 9. Practical Tips for Final Projects 

### 10. Question Answering and the Default Final Project

## Week 6 (ì§„í–‰ ì¤‘)

### 11. ConvNets for NLP 

### 12. Information from parts of words: Subword Models

## Week 7 (ì§„í–‰ ì¤‘)

### 13. Modeling contexts of use: Contextual Representations and Pretraining 

### 14. Transformers and Self-Attention For Generative Models

## Week 8 (ì§„í–‰ ì¤‘)

### 15. Natural Language Generation 

### 16. Reference in Language and Coreference Resolution

## Week 9 (ì§„í–‰ ì¤‘)

### 17. Multitask Learning: A general model for NLP?

### 18. Constituency Parsing and Tree Recursive Neural Networks 

### 19. Future of NLP + Deep Learning 


